{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I Mathematical Foundations\n",
    "## Machine learning's three core concepts: data, model, learning\n",
    "\n",
    "1. Data\n",
    "    * Goal of machine learning is to design general-purpose methodologies to extract valuable patterns from data\n",
    "    * Ideally without much domain-specific expertise\n",
    "2. Models: Related to the process that generates data\n",
    "3. Learning: Automatically find patterns and structure in data by optimizing the parameters of the model\n",
    "    \n",
    "\n",
    "### 1.1 Finding Words for Intuition\n",
    "* Algorithm\n",
    "1. Predictors: \"Machine learning algorithm\" is a system that makes predictions based on input data\n",
    "2. Training: \"Machine learning algorithm\" is a system taht adapts some internal parameters of the predictor so taht it performs well on future unseen input data\n",
    "    \n",
    "\n",
    "* In the book, consider data as vectors\n",
    "* Vectors\n",
    "1. Array of numbers(CS)\n",
    "2. Arrow with a direction and magnitude(Physics)\n",
    "3. Object that obeys addition and scaling(Math)\n",
    "\n",
    "* Good model can be used to predict without performing real-world experiments\n",
    "\n",
    "* Learning compoennt\n",
    "    * Training a model: Use the data to optimize parameters of the model to evaluae how well the model predicts the training data\n",
    "    * Interested in the model to perform well on unseen data\n",
    "\n",
    "### 1.2 Two ways to read this book: Bottom-up or top-down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Linear Algebra\n",
    "\n",
    "* Algebra: Construct a set of objects and a set of rules to manipulate these objects\n",
    "* Linear algebra: Study of vectors and certain rules to manipulate vectors\n",
    "* Vectors: speical objects that can be added together and multiplied by scalars\n",
    "    1. Geometric vectors\n",
    "    2. Polynomials\n",
    "    3. Audio signals\n",
    "    4. Elements of R^n\n",
    "\n",
    "* Linear algebra focuses on the similarities between these vector concepts\n",
    "* Largely focus on vectors in R^n\n",
    "* Finite dimensional vector spaces\n",
    "\n",
    "* Idea of \"Closure\" resulting in vector space which underlies much of machine learning\n",
    "\n",
    "## 2.1 Systems of Linear Equations\n",
    "* A real valued system of linear equations: No, exactly one, or infinitely many solutions\n",
    "* Linear equations with two variables) each linear equation defiens a line on the x1x2-plane\n",
    "    * Intersection can be a line, a, point or empty\n",
    "* Three varaibles) each linear equation determines a plane in three-dimensional space\n",
    "    * Solution set can be a plane, a line, a point, or empty\n",
    "* Use compact notations a.k.a. matrices\n",
    "\n",
    "## 2.2 Matrices\n",
    "* Central role in linear algebra\n",
    "* Used to compactly represent systems of linear equations, but also represent linear functions(linear mappings)\n",
    "\n",
    "* Definition: \n",
    "![](./0122_16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Matrix Addition and Multiplication\n",
    "![](./0122_17.jpg)\n",
    "\n",
    "* To compute element cij, we multiply the elements of the ith row of A with the jth column of B and sum them up\n",
    "* Call this dot product\n",
    "* Remark. Matrices can only be multiplied if their neighboring dimensions match\n",
    "* Remark. Matrix multiplication is not defined as an element-wise operation on matrix elements\n",
    "\n",
    "* Definition 2.2. Identity matrix\n",
    "![](./0122_18.jpg)\n",
    "\n",
    "* Properties of matrices\n",
    "![](./0122_19.jpg)\n",
    "\n",
    "### 2.2.2. Inverse and Transpose\n",
    "* Definition 2.3 (Inverse). Consider a square matrix A ∈ R nxn. Let matrix B ∈ R nxn have the property that AB = I = BA. B is the inverse of A and denoted by A^-1\n",
    "* Not every matrix A possesses an inverse A^-1\n",
    "* A: regular, invertible, nonsingular\n",
    "* When inverse exists, it is unique\n",
    "\n",
    "* Definition 2.4 (Transpose). For A ∈ R mxn the matrix B ∈ R nxm with bij = aji is called the transpose of A. We write B = A^T\n",
    "![](./0122_19.jpg)\n",
    "\n",
    "* Definition 2.5 (Symmetric Matrix). A matrix A ∈ R nxn is symmetric if A = A^T\n",
    "* Only nxn can be symmetric, or square matrices\n",
    "* If A is invertible, then so is A^T, and (A^-1)T = (A^T)-1 = A^-T\n",
    "\n",
    "### 2.2.3 Multiplication by a Scalar\n",
    "λ ∈ R mxn and λ ∈ R. Then λA = K, Kij = λaij\n",
    "![](./0122_19.jpg)\n",
    "\n",
    "### 2.2.4 Compact Representation of Systems of Linear Equations\n",
    "* Compactly represented as **Ax = b**\n",
    "* The product Ax is a lienar combination of the columns of A\n",
    "\n",
    "\n",
    "## 2.3 Solving Systems of Linear Equations\n",
    "aij ∈ R and bi ∈ R are known and xj are unknowns\n",
    "\n",
    "* System has 2 equations and 4 unknowns\n",
    "* Solution is [42, 8, 0, 0]^T called particular solution or linear equations\n",
    "* Not the only solution. To capture other solutions, need to be creative in generating 0 in a non-trivial way using the columns of th ematrix: Adding 0 to our special solution does not change the special solution\n",
    "* Express the third column using the first two columns \n",
    "\n",
    "*Remark: General approach \n",
    "1. Find a particular solution to Ax = b\n",
    "2. Find all solutions to Ax = 0\n",
    "3. Combine the solutions from steps 1. and 2. to the general solution\n",
    "\n",
    "* General equation systems are not of this simple form\n",
    "* Fortunately, constructive algorithmic way of transforming any system of linear equations into this particularly simple form\n",
    "* Gaussian elimination\n",
    "* Key: Elementary transformations of systems of lienar equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Elementary Transformations\n",
    "\n",
    "* Key of solving a system: elementary transformations that keep the solution set the same, but that transform the equation system into a simpler form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exchange of two equations (rows in the matrix representing the system of equations)\n",
    "* Multiplication of an equation (row) with a constant λ ∈ R \\{0}\n",
    "* Addition of two equations(rows)\n",
    "\n",
    "\n",
    "* Remark: Row-Echelon Form\n",
    "    * A matrix is in row-echelon form if \n",
    "        * All rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on top of rows that contain only zeros\n",
    "        * Looking at nonzero rows only, the first nonzero number from the left(also called the pivot or the leading coefficient) is always strictly to the right of the pivot of the row above it\n",
    "        \n",
    "        \n",
    "* Remark: Basic and Free Varaibles\n",
    "    * The variables corresponding to the pivots in the row-echelon form are called basic variables and others are free variables\n",
    "\n",
    "\n",
    "* Remark: Obtaining a particular solution\n",
    "    * Row-echelon helpful when need to determine a particular solution\n",
    "    * express the right hand side of the equation system using the pivot columns \n",
    "\n",
    "\n",
    "* Remark: Reduced Row Echelon Form\n",
    "    * It is in row-echelon form\n",
    "    * Every pivot is 1\n",
    "    * The pivot is the only nonzero entry in its column\n",
    "    * Allows us to determine the general solution of a system of linear equations in a straightforward way\n",
    "\n",
    "\n",
    "* Remark : Gaussian Elimination(Algorithm that performs elementary transformations to bring a system of linear equations into reduced row-echelon form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 The Minus-1 Trick\n",
    "* Practical trick for reading out the solutions x of a homogeneous system of linear equations Ax = 0, where A ∈ R kxn, x ∈ Rn\n",
    "* To start, assume that A is in reduced row-echelon form without any rows that just contain zeros\n",
    "* Extend this matrix to an nxn matrix A by adding n-k rows of the form [0 ... 0 -1 0 ... 0] so that the diagonal of tha augmented matrix A contains either 1 0r -1\n",
    "* Columns of A that contain -1 as pivots are solutions of the homogeneous equation system Ax = 0\n",
    "* Columns form a basis of the solution space of Ax = 0 a.k.a. kernel or null space\n",
    "\n",
    "### Calculating the Inverse\n",
    "* Have to find a matrix X that satisfies AX = I, then X = A^-1\n",
    "* If we bring the augmented equation system into reduced row-echelon form, we can reach out the inverse on the righ thand side of the equation system\n",
    "* Determining the inverse of a matrix = solving systems of linear equations\n",
    "\n",
    "### 2.3.4 Algorithms for Solving a System of Linear Equations\n",
    "* Assume that a solution exists\n",
    "* Can determine the inverse A^-1 such that the solution of Ax = b is given as x = A^-1b\n",
    "* However, only possible if A is a square matrix and invertible\n",
    "\n",
    "\n",
    "* Disadvantage: Requires many computaitons for the matrix-matrix product and computing the inverse of ATA\n",
    "* Discuss alternative approaches to solving systems of linear equations\n",
    "\n",
    "\n",
    "* **Gaussian elimination**\n",
    "* Checking whether a set of vectors is linearly independent, computing the inverse of a matrix, computing the rank of a matrix, and determing a basis of a vector space\n",
    "* Intuitive and constructive way to solve a system of linear equations with tousands of variables\n",
    "* For systems with millions of variables, impractical\n",
    "\n",
    "\n",
    "* In practice, systems of many linear equations are solved indirectly, by either stationary iterative methods, such as the Richardson method, the Jacobi method, and the Gauss-Seidel method, and the successive over-relaxation method, or Krylov subspace methods, generalized minimal residual, or biconjugate gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Spaces\n",
    "* Systems of linear equations can be compactly represented using matrix-vector notation\n",
    "* Have a closer look at ector spaces, a structured space in which vectors live\n",
    "\n",
    "### 2.4.1 Groups\n",
    "![](./0123_01.jpg)\n",
    "\n",
    "* Definition 2.8 General Linear Group. The set of regular matrices A ∈ R nxn is a group with respect to matrix multiplication as defined in (2.13)\n",
    "* General linear group GL(n, R)\n",
    "\n",
    "### 2.4.2 Vector Spaces\n",
    "![](./0123_02.jpg)\n",
    "\n",
    "* Vectors: elements x ∈ V\n",
    "* Zero vector 0 = [0,...,0]T (V, +)\n",
    "* Vector addition: Inner operation +\n",
    "* Scalars: Elements scalar ∈ R\n",
    "\n",
    "* Remark. A vector multiplication ab, a, b ∈ Rn is not defined\n",
    "* Array multiplication is common to many programming languages but makes matematically limited sense using the standard rules for matrix multiplication\n",
    "    * By treating vectors as n X 1 matrices, can use the matrix multiplication as defined\n",
    "    * However, dimensions of the vectors do not match\n",
    "\n",
    "\n",
    "* Remark. Denote a vector space (V, +, .) by V when + and . are the standard vector addition and scalar multiplicaiton\n",
    "* Use the notation x ∈ V for vectors in V to simplify notation\n",
    "\n",
    "\n",
    "* Remark. Vector spaces Rn, Rnx1, R1xn are only different in the way we write vectors\n",
    "* Write x to denote a column vector, and a row vector transpose of x\n",
    "\n",
    "\n",
    "### 2.4.3 Vector Subspaces\n",
    "* Vector subspaces: Sets contained in the original vector space with the property that when we perform vector space operations on elements within this subspce, we will never leave it\n",
    "* In this sense, they are closed\n",
    "\n",
    "![](./0123_03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Linear Independence\n",
    "\n",
    "* Concepts of lienar combinations and linear dependence\n",
    "\n",
    "![](./0123_04.jpg)\n",
    "\n",
    "* One of the most importanc econcepts in linear algebra\n",
    "* A set of linearly independent vectors consists of vectors that have no redundancy\n",
    "    * If remove any of vectors, we will lost something\n",
    "    \n",
    "\n",
    "* Remark. The following properties are useful to find out whether vectors are linearly independent:\n",
    "    * k vectors are either linearly dependent or linearly indpendent\n",
    "    * IF at least one of the vectors x1, ..., xk is 0 then they are linearly dependent. The same holds if two vectors are identical\n",
    "    * The vectors { x1, ..., xk : xi != 0, i = 1, ..., k}, k>= 2, are linearly dependent if and only if (at least) one of them is a linear combination of the others\n",
    "    * If one vector is a multiple of another vectors, i.e., xi = λ ∈ R then the set { x1, ..., xk : xi != 0, i = 1, ..., k} is linearly dependent\n",
    "    * A practical way of checking whether vectors x1, ..., xk ∈ V are linearly independent is to use Gaussian elimination\n",
    "        * Write all vectors as columns of a matrix A and perform Gaussian elimination until the matrix is in row echelon form\n",
    "    * All column vectors are linearly independent if and only if all columns are pivot columns\n",
    "        * If at least one non-pivot column, the columns are linearly dependent\n",
    "\n",
    "![](./0123_05.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
